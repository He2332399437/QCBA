import os
import json
import random
import numpy as np
import scipy.io as sio

step_size = 512 
overlap_rate = 0.25 
sample_length = 512  

train_json_file = 'D:/yolo/code/qwen/output/data/features/train.json'
test_json_file = 'D:/yolo/code/qwen/output/data/features/test.json' 

os.makedirs(os.path.dirname(train_json_file), exist_ok=True)
os.makedirs(os.path.dirname(test_json_file), exist_ok=True)

FAULT_LABELS = {
    "State Stability represents normal signals": 0,
    "Micro-Corrosion of Shaft Core": 1,
    "Ball Ring Light Fracture": 2,
    "Frame Edge Initial Damage": 3,
    "Core Component Cracking": 4,
    "Roller Destruction": 5,
    "Outer Layer Peeling": 6,
    "Shaft Core Depression": 7,
    "Column Disk Fragmentation": 8,
    "Ring Wall Collapse": 9,
}

def extract_fault_label(folder_name):
    for fault_type, label in FAULT_LABELS.items():
        if fault_type in folder_name:
            print(f"âœ… åŒ¹é…æˆåŠŸ: {folder_name} -> {fault_type} -> ç±»åˆ« {label}")
            return label
    print(f"âŒ æœªåŒ¹é…åˆ°ç±»åˆ«: {folder_name}")
    return None 

def calculate_features(signal):
    arr = np.array(signal, dtype=float)
    mean_val = np.mean(arr)
    std_val  = np.std(arr)
    rms_val  = np.sqrt(np.mean(arr**2))
    abs_mean = np.mean(np.abs(arr))
    peak_val = np.max(np.abs(arr))
    skew_val = (np.mean((arr - mean_val)**3) / (std_val**3)) if std_val != 0 else 0
    kurt_val = (np.mean((arr - mean_val)**4) / (std_val**4)) if std_val != 0 else 0
    crest_factor   = (peak_val / rms_val) if rms_val != 0 else 0
    impulse_factor = (peak_val / abs_mean) if abs_mean != 0 else 0

    return {
        "Mean": mean_val,
        "StdDev": std_val,
        "RMS": rms_val,
        "AbsMean": abs_mean,
        "Peak": peak_val,
        "Skew": skew_val,
        "Kurt": kurt_val,
        "CrestFactor": crest_factor,
        "ImpulseFactor": impulse_factor
    }

def process_mat_file(file_path, label):
    try:
        mat_data = sio.loadmat(file_path)
        possible_keys = ["X118_DE_time", "X222_DE_time", "X169_DE_time", "X130_DE_time", "X105_DE_time",
                         "X197_DE_time", "X234_DE_time", "X185_DE_time", "X209_DE_time", "X097_DE_time"]
        signal_key = next((key for key in possible_keys if key in mat_data), None)

        if signal_key is None:
            print(f"âš ï¸ {file_path} æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä¿¡å·é”®ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ï¼")
            return []

        time_series = mat_data[signal_key].flatten()
        if len(time_series) < sample_length:
            print(f"âš ï¸ {file_path} ä¿¡å·é•¿åº¦è¿‡çŸ­ ({len(time_series)} < {sample_length})ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ï¼")
            return []

        segments = []
        overlap = int(step_size * overlap_rate)
        for start in range(0, len(time_series) - sample_length + 1, step_size - overlap):
            segment = time_series[start: start + sample_length]
            feature_values = calculate_features(segment)  
            segments.append({
                "signal": segment.tolist(),
                "features": feature_values, 
                "label": str(label)
            })

        print(f"âœ… {file_path} å¤„ç†æˆåŠŸï¼Œå…±æå– {len(segments)} ç‰‡æ®µ")
        return segments

    except Exception as e:
        print(f"âŒ å¤„ç† {file_path} å¤±è´¥: {e}")
        return []

def process_folder(input_folders, train_json_file, test_json_file):
    file_paths = []

    for folder in input_folders:
        label = extract_fault_label(folder) 
        for root, _, files in os.walk(folder):
            for file in files:
                if file.endswith('.mat'):
                    file_path = os.path.join(root, file)
                    file_paths.append((file_path, label))

    print(f"ğŸ“‚ å¤„ç†å‰æ‰€æœ‰æ–‡ä»¶: {len(file_paths)}")

    all_data = []
    for idx, (file_path, label) in enumerate(file_paths):
        print(f"Processing file {idx + 1}/{len(file_paths)}: {file_path}")
        segments = process_mat_file(file_path, label)
        all_data.extend(segments)

    category_data = {str(i): [] for i in range(10)}
    for entry in all_data:
        category_data[entry["label"]].append(entry)

    train_data, test_data = [], []

    for label, data in category_data.items():
        random.shuffle(data)  
        split_idx = int(0.9 * len(data)) 
        train_data.extend(data[:split_idx])
        test_data.extend(data[split_idx:])

    for label in range(10):
        label = str(label)
        if len(category_data[label]) == 0:
            print(f"âš ï¸ ç±»åˆ« {label} æ²¡æœ‰æ•°æ®ï¼")
        else:
            print(f"âœ… ç±»åˆ« {label}: è®­ç»ƒ {len([d for d in train_data if d['label'] == label])} | æµ‹è¯• {len([d for d in test_data if d['label'] == label])}")

    random.shuffle(train_data)
    random.shuffle(test_data)

    with open(train_json_file, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=4)

    with open(test_json_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… è®­ç»ƒæ•°æ® {len(train_data)} æ¡ï¼Œå·²ä¿å­˜è‡³ {train_json_file}")
    print(f"âœ… æµ‹è¯•æ•°æ® {len(test_data)} æ¡ï¼Œå·²ä¿å­˜è‡³ {test_json_file}")

if __name__ == '__main__':
    input_folders = [
        'dataset_root'
           ]

    process_folder(input_folders, train_json_file, test_json_file)
